# ğŸ§  CENG 442 â€“ Assignment 1

### Azerbaijani Text Preprocessing + Word Embeddings (Domain-Aware)

---

## 1ï¸âƒ£ Data & Goal

Five Azerbaijani sentiment-analysis datasets were cleaned and relabeled using three sentiment levels
(**Negative = 0.0**, **Neutral = 0.5**, **Positive = 1.0**).
Each dataset was converted into a two-column Excel file (`cleaned_text`, `sentiment_value`), then merged into a single corpus (`corpus_all.txt`).
The goal is to train **Word2Vec** and **FastText** embeddings and compare their performance across domains (news, social, reviews, general).

---

## 2ï¸âƒ£ Preprocessing (Rules Summary)

All cleaning and normalization were performed automatically using `main.py`.
Key rules:

* Azerbaijani-specific lower-casing (`Ä°â†’i`, `Iâ†’Ä±`), Unicode NFC normalization.
* Replace URLs â†’ `URL`, emails â†’ `EMAIL`, phones â†’ `PHONE`, @mentions â†’ `USER`.
* Hashtag split: `#QarabagIsBack â†’ qarabag is back`.
* Emoji mapping ğŸ™‚â†’`EMO_POS`, â˜¹â†’`EMO_NEG`.
* Numbers â†’ `<NUM>`; collapse â‰¥ 3 repeated letters â†’ 2 (`cooool â†’ coool`).
* Negation scope: mark next 3 tokens with `_NEG` after negators (`yox`, `deyil`, etc.).
* Domain detection + `dom<domain>` prefix per line.
* Remove duplicates and empty rows.

**Result:** â‰ˆ 124 k sentences in `corpus_all.txt`.

---

## 3ï¸âƒ£ Mini Challenges (Implemented)

* âœ… Hashtag splitting (`#QarabagIsBack â†’ qarabag is back`)
* âœ… Emoji mapping ğŸ™‚/â˜¹ â†’ EMO_POS / EMO_NEG
* âœ… Negation scope annotation (`yaxÅŸÄ± deyil â†’ yaxÅŸÄ±_NEG`)
* âœ… Stopword research (AZâ€“TR comparison; 20 candidate stopwords)
* âœ… De-asciification (`cox â†’ Ã§ox`, `yaxsi â†’ yaxÅŸÄ±`)

---

## 4ï¸âƒ£ Domain-Aware Processing

Domains were detected using lightweight regex rules and tagged in the corpus.

| Domain  | Trigger Patterns                        |
| :------ | :-------------------------------------- |
| news    | apa, trend, azertac, reuters, bloomberg |
| social  | @, #, ğŸ˜‚, ğŸ˜, ğŸ™‚                        |
| reviews | azn, manat, qiymÉ™t, ulduz               |
| general | default case                            |

**Examples**

```
domreviews  Ã§ox yaxÅŸÄ± mÉ™hsuldur
domsocial   ğŸ˜‚ bu gÃ¼n Ã§ox gÃ¶zÉ™l idi
```

---

## 5ï¸âƒ£ Embeddings â€“ Training & Results

### Training Settings

| Parameter     | Value              |
| :------------ | :----------------- |
| vector_size   | 300                |
| window        | 5                  |
| min_count     | 3                  |
| sg            | 1 (Skip-Gram)      |
| epochs        | 10                 |
| negative      | 10 (Word2Vec only) |
| min_n / max_n | 3 / 6 (FastText)   |

---

### Lexical Coverage

| Dataset              | Word2Vec | FastText |
| :------------------- | :------: | :------: |
| labeled-sentiment    |   0.932  |   0.932  |
| test__1_             |   0.987  |   0.987  |
| train__3_            |   0.990  |   0.990  |
| train-00000-of-00001 |   0.943  |   0.943  |
| merged_dataset       |   0.949  |   0.949  |

---

### Synonym / Antonym Similarities

| Metric                 | Word2Vec | FastText |
| :--------------------- | :------: | :------: |
| Synonyms               |   0.355  |   0.445  |
| Antonyms               |   0.347  |   0.429  |
| Separation (Syn â€“ Ant) |   0.008  |   0.016  |

---

### Nearest Neighbors (Examples)

```
'yaxÅŸÄ±' â†’ W2V: ['<RATING_POS>', 'yaxshi', 'yaxsi']
'yaxÅŸÄ±' â†’ FT: ['yaxÅŸÄ±l', 'yaxÅŸÄ±lÄ±q', 'yaxÅŸÄ±ca']
'ucuz'  â†’ ['ucuzdur', 'ucuzluÄŸa', 'ucuza']
'mÃ¼kÉ™mmÉ™l' â†’ ['mÃ¶hÂ­tÉ™ÅŸÉ™m', 'mÃ¼kÉ™mmÉ™ldi', 'mÃ¼kÉ™mmal']
```

---

## 6ï¸âƒ£ Reproducibility

| Component | Version    |
| :-------- | :--------- |
| Python    | 3.11.9     |
| pandas    | 2.3        |
| gensim    | 4.4        |
| numpy     | 1.26       |
| OS        | Windows 11 |

**Run sequence:**

```bash
python main.py
python train_embeddings.py
python compare_models.py
```

---

## 7ï¸âƒ£ Conclusions

* **FastText** outperforms on morphologically rich forms (e.g., *yaxsi â†” yaxÅŸÄ±*) due to subword modeling.
* **Word2Vec** is more stable for frequent tokens but fails for OOV words.
* Overall coverage 93 â€“ 99 % â†’ excellent tokenization and corpus consistency.
* Future work: domain-specific training (e.g., `domnews` vs `domsocial`) and cross-domain drift analysis.

---

## ğŸ‘¥ Group Members

* Efe Åahin
* Selin SargÄ±n

---

## ğŸ“¦ Repository Structure

```
main.py
train_embeddings.py
compare_models.py
requirements.txt
labeled-sentiment_2col.xlsx
test__1__2col.xlsx
train__3__2col.xlsx
train-00000-of-00001_2col.xlsx
merged_dataset_CSV__1__2col.xlsx
corpus_all.txt
```

**Embeddings (Google Drive):**
[https://drive.google.com/drive/folders/](https://drive.google.com/drive/folders/)<YOUR_DRIVE_LINK_HERE>
